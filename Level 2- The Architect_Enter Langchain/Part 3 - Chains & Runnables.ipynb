{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417fd6e1-6d81-463d-9b3a-442a8432cb2b",
   "metadata": {},
   "source": [
    "# Part 3: Chains and Runnables in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc324a33-a625-485c-aa84-34c07ae8a136",
   "metadata": {},
   "source": [
    "## Chains: A Look Back\n",
    "\n",
    "Imagine LangChain in its early days, when developers were just starting to figure out how to build applications around Large Language Models (LLMs). There was a clear need to string together different operations. You'd want to take some user input, pass it through a prompt, then send that to an LLM, and maybe even process the LLM's output further. This is precisely what **Chains** were designed to do.\n",
    "\n",
    "### What Chains Were and Their Original Purpose\n",
    "\n",
    "In essence, **Chains were sequences of calls, or \"links,\" that processed data in a predefined order.** Each link in the chain would take an input, perform an operation, and then pass its output as the input to the next link. Think of it like an assembly line, where each station performs a specific task.\n",
    "\n",
    "The original purpose of Chains was to simplify the development of multi-step LLM applications. Instead of manually managing the input and output of each component, you could define a chain, and LangChain would handle the flow for you.\n",
    "\n",
    "### Simple Chain Examples\n",
    "\n",
    "Let's look at a very basic example of what a chain might have looked like. Remember, we're not going to run this code as Chains are largely deprecated, but it helps to understand the concept.\n",
    "\n",
    "```python\n",
    "# This is conceptual code to illustrate old Chains\n",
    "# Do NOT run this code as it uses deprecated patterns\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.chains import LLMChain # Deprecated import\n",
    "\n",
    "# 1. Define a Prompt Template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "# 2. Define the LLM (hypothetically)\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# 3. Create an LLMChain\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm) # This is how it used to be\n",
    "\n",
    "# To use it:\n",
    "# response = llm_chain.run(\"colorful socks\")\n",
    "# print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d6f6d-c3b4-4afc-b207-270536017160",
   "metadata": {},
   "source": [
    "In this conceptual example, the `LLMChain` took a prompt and an LLM, effectively chaining them together. You'd give it the product, it would format the prompt, send it to the LLM, and return the LLM's response.\n",
    "\n",
    "You could even create more complex chains, for instance, by adding a simple parsing step:\n",
    "\n",
    "```python\n",
    "# This is conceptual code to illustrate old Chains\n",
    "# Do NOT run this code as it uses deprecated patterns\n",
    "\n",
    "# from langchain.chains import SimpleSequentialChain # Deprecated import\n",
    "\n",
    "# prompt1 = PromptTemplate(...) # First prompt\n",
    "# llm1 = OpenAI(...) # First LLM\n",
    "# chain1 = LLMChain(prompt=prompt1, llm=llm1)\n",
    "\n",
    "# prompt2 = PromptTemplate(...) # Second prompt\n",
    "# llm2 = OpenAI(...) # Second LLM\n",
    "# chain2 = LLMChain(prompt=prompt2, llm=llm2)\n",
    "\n",
    "# overall_chain = SimpleSequentialChain(chains=[chain1, chain2]) # Chaining them\n",
    "# response = overall_chain.run(\"some input\")\n",
    "```\n",
    "\n",
    "Here, `SimpleSequentialChain` allowed you to run multiple chains in sequence, where the output of one became the input of the next.\n",
    "\n",
    "### Key Limitations that Led to Their Replacement\n",
    "\n",
    "While Chains were a good start, they had some significant limitations that became apparent as LangChain grew and the complexity of LLM applications increased:\n",
    "\n",
    "1.  **Lack of Flexibility:** Chains were often rigid. If you wanted to do something that wasn't a simple sequential flow, like branching logic, parallel execution, or handling multiple inputs/outputs, it became cumbersome or impossible.\n",
    "2.  **Limited Error Handling and Debugging:** Debugging complex chains could be challenging. It was often hard to pinpoint where an error occurred or to inspect intermediate results.\n",
    "3.  **No Streaming or Async Support:** As LLMs became more responsive, the need for streaming outputs (like seeing words appear one by one) and asynchronous execution (running multiple things at once without blocking) became crucial. Chains weren't built with this in mind.\n",
    "4.  **Poor Compositionality:** While they introduced the *concept* of composition, the way Chains were designed often led to a hierarchical, nested structure that was hard to reason about and modify. It wasn't always clear how to compose different types of operations effectively.\n",
    "5.  **Performance Issues:** Without native support for concurrent execution, performance could suffer in scenarios where multiple LLM calls or other operations could theoretically run in parallel.\n",
    "\n",
    "### How Chains Introduced the Concept of Component Composition\n",
    "\n",
    "Despite their limitations, Chains were instrumental in introducing a fundamental idea that still underpins LangChain: **component composition**. They showed us the power of breaking down complex tasks into smaller, manageable pieces (like prompts, LLMs, parsers) and then assembling those pieces into a larger workflow. This modularity is key to building scalable and maintainable LLM applications.\n",
    "\n",
    "### This is Where Runnables Come In...\n",
    "\n",
    "The LangChain team recognized these limitations and completely reimagined how components should interact. They wanted a system that was more flexible, more powerful, and inherently supported modern application development patterns like streaming and asynchronicity.\n",
    "\n",
    "And that, my friends, is why **Runnables** were born\\! They are the evolution of component composition in LangChain, designed to address all the shortcomings of Chains and provide a robust framework for building almost any LLM application imaginable.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dae41e-949f-444b-9679-5125299f3b04",
   "metadata": {},
   "source": [
    "### Introduction: The Building Blocks of LangChain\n",
    "\n",
    "The best way to think of a **Runnable** is as a single \"unit of work.\" Every major component in LangChain—from prompt templates to language models to output parsers—is a Runnable. This is what makes LangChain so modular and powerful. This standardized interface is the secret sauce that allows for such seamless composition.\n",
    "\n",
    "This design philosophy means you can take these individual units of work and chain them together in creative ways to build complex applications. The \"glue\" that holds them all together is the **Runnable interface**, which provides a standard set of methods (`invoke`, `batch`, `stream`) to execute these units of work. Because every component speaks the same \"language,\" you can be confident that they will connect predictably, allowing you to build robust and scalable AI systems.\n",
    "\n",
    "### Core Types of Runnables\n",
    "\n",
    "Here are practical examples of the four fundamental Runnable types we discussed.\n",
    "\n",
    "#### 1. `RunnableLambda`\n",
    "\n",
    "This is how you turn any regular Python function into a LangChain component that you can use in a chain. It's perfect for simple, custom transformations, data validation, or any ad-hoc logic you need to inject into a sequence. It provides an escape hatch to use traditional code within the declarative LangChain Expression Language (LCEL) framework.\n",
    "\n",
    "**Use Case:** You want to create a simple step in your chain that takes a list of words and joins them into a single sentence.\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# A simple python function\n",
    "def join_words(words: list) -> str:\n",
    "  \"\"\"Takes a list of words and joins them with a space.\"\"\"\n",
    "  return \" \".join(words)\n",
    "\n",
    "# Wrap the function in a RunnableLambda\n",
    "join_words_runnable = RunnableLambda(join_words)\n",
    "\n",
    "# Now you can use it like any other LangChain component\n",
    "input_words = [\"LangChain\", \"runnables\", \"are\", \"powerful!\"]\n",
    "result = join_words_runnable.invoke(input_words)\n",
    "\n",
    "print(result)\n",
    "# Expected Output: LangChain runnables are powerful!\n",
    "```\n",
    "\n",
    "#### 2. `RunnableSequence`\n",
    "\n",
    "This is the most common way to build chains. It executes a series of Runnables in order, where the output of one becomes the input for the next. The `|` (pipe) operator is the elegant shorthand for creating a sequence, making the flow of data intuitive and easy to read. Data flows from left to right, often changing its type as it passes through each component.\n",
    "\n",
    "**Use Case:** Create a simple chain that takes a topic, formats it into a prompt, and gets a response from a model.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the components of our chain\n",
    "prompt_template = ChatPromptTemplate.from_template(\"Tell me a fact about {topic}.\")\n",
    "model = ChatOpenAI() # Note: Requires OPENAI_API_KEY to be set\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create the sequence using the pipe operator\n",
    "# Data flow: dict -> PromptValue -> AIMessage -> str\n",
    "chain = prompt_template | model | output_parser\n",
    "\n",
    "# Execute the chain\n",
    "result = chain.invoke({\"topic\": \"the moon\"})\n",
    "\n",
    "print(result)\n",
    "# Expected Output: A fact about the moon, e.g., \"The Moon is drifting away from the Earth at a rate of about 3.8 cm per year.\"\n",
    "```\n",
    "\n",
    "#### 3. `RunnableParallel`\n",
    "\n",
    "This allows you to execute multiple Runnables at the same time with the *same input*. The results are returned in a dictionary, where the keys are the ones you define. It's great for when you need to run independent operations on the same piece of data, often leading to significant performance gains as the operations can be run concurrently.\n",
    "\n",
    "**Use Case:** You have a user's question and you want to simultaneously get a direct answer from an LLM and also generate some potential follow-up questions.\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# A chain to get a direct answer\n",
    "answer_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Answer the following question: {question}\")\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "\n",
    "# A chain to generate follow-up questions\n",
    "follow_up_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Generate 3 follow-up questions for: {question}\")\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "\n",
    "# Create a parallel runnable. The keys 'answer' and 'follow_up' will be the keys in the output dictionary.\n",
    "# Both chains will receive the same input: {\"question\": \"...\"}\n",
    "combined_chain = RunnableParallel(\n",
    "    answer=answer_chain,\n",
    "    follow_up=follow_up_chain\n",
    ")\n",
    "\n",
    "# Execute the parallel chain\n",
    "result = combined_chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "\n",
    "print(result)\n",
    "# Expected Output:\n",
    "# {\n",
    "#   'answer': AIMessage(content='The capital of France is Paris.'),\n",
    "#   'follow_up': AIMessage(content='1. What are some famous landmarks in Paris?\\n2. What is the history of the Eiffel Tower?\\n3. What is the official language spoken in France?')\n",
    "# }\n",
    "```\n",
    "\n",
    "#### 4. `RunnablePassthrough`\n",
    "\n",
    "This is a simple but very useful utility. It takes the input and passes it through unchanged. It's often used with `RunnableParallel` to pass along an original input field alongside new, computed values. This is crucial for maintaining context that might be needed by later steps in a more complex chain.\n",
    "\n",
    "**Use Case:** You want to answer a question but also keep the original question in the final output for context.\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# The main chain that will answer the question\n",
    "answer_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Tell me a fact about {topic}.\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# We use RunnablePassthrough to pass the original 'topic' through.\n",
    "# The 'answer' key will be populated by our answer_chain.\n",
    "chain = RunnableParallel(\n",
    "    answer=answer_chain,\n",
    "    original_topic=RunnablePassthrough()\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "result = chain.invoke({\"topic\": \"the sun\"})\n",
    "\n",
    "print(result)\n",
    "# Expected Output:\n",
    "# {\n",
    "#   'answer': 'The Sun accounts for about 99.86% of the total mass of the Solar System.',\n",
    "#   'original_topic': {'topic': 'the sun'}\n",
    "# }\n",
    "```\n",
    "\n",
    "### The Runnable Interface: `invoke`, `batch`, and `stream`\n",
    "\n",
    "This is the standard API for executing any Runnable. Having a consistent API means you don't have to think about *how* to run a component; you only need to decide whether you're processing a single item, a batch of items, or streaming the output.\n",
    "\n",
    "#### `invoke()`: Single Input, Single Output\n",
    "\n",
    "This is the most straightforward method. You use it when you have one input and you want to get a single, complete output back. It's a synchronous, blocking call; your code will wait until the entire operation is finished before moving to the next line.\n",
    "\n",
    "**Use Case:** A simple translation task.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Translate '{text}' from English to French.\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a simple chain\n",
    "translation_chain = prompt | model\n",
    "\n",
    "# Use invoke for a single input/output operation\n",
    "result = translation_chain.invoke({\"text\": \"I love programming.\"})\n",
    "\n",
    "print(result.content)\n",
    "# Expected Output: \"J'adore la programmation.\"\n",
    "```\n",
    "\n",
    "#### `batch()`: Multiple Inputs, Multiple Outputs\n",
    "\n",
    "This is for efficiency. When you have a list of inputs, `batch()` processes them in parallel (where possible), which is much faster than calling `invoke()` in a loop. LangChain's runtime can make concurrent API calls to the model provider, dramatically reducing the total execution time for large datasets.\n",
    "\n",
    "**Use Case:** Translating a list of phrases.\n",
    "\n",
    "```python\n",
    "# (Continuing from the previous example)\n",
    "\n",
    "# A list of inputs\n",
    "phrases = [\n",
    "    {\"text\": \"Hello world\"},\n",
    "    {\"text\": \"How are you?\"},\n",
    "    {\"text\": \"Good morning\"}\n",
    "]\n",
    "\n",
    "# Use batch to process all inputs at once, which is much faster than a for loop.\n",
    "results = translation_chain.batch(phrases)\n",
    "\n",
    "for res in results:\n",
    "    print(res.content)\n",
    "\n",
    "# Expected Output:\n",
    "# \"Bonjour le monde\"\n",
    "# \"Comment allez-vous ?\"\n",
    "# \"Bonjour\"\n",
    "```\n",
    "\n",
    "#### `stream()`: Chunked Outputs\n",
    "\n",
    "This is for real-time applications, like chatbots. Instead of waiting for the full response, `stream()` returns an iterator that yields the output in chunks as it's being generated by the model. This creates a much better user experience, as the user sees immediate feedback instead of a loading spinner, improving the perceived performance of the application.\n",
    "\n",
    "**Use Case:** Streaming a story from an LLM so the user sees the text appearing word-by-word.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short story about a brave knight named {name}.\")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "story_chain = prompt | model | parser\n",
    "\n",
    "# Use stream to get the output in chunks\n",
    "# The 'for' loop will print each chunk as it arrives, giving a real-time effect.\n",
    "for chunk in story_chain.stream({\"name\": \"Arthur\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# Expected Output: A story about Sir Arthur will be printed to the console\n",
    "# token by token, giving a real-time streaming effect.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b120a2-8624-4ef2-b5ed-f308d02d841a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
