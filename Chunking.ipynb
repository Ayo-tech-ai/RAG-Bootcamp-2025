{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWzwCjtWtIG4Hf7XY2R3og",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsk-ai/RAG-Bootcamp-2025/blob/main/Chunking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/concepts/text_splitters/"
      ],
      "metadata": {
        "id": "WE4TyseLLWg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Text Splitters (Chunking)"
      ],
      "metadata": {
        "id": "7ewhNc58Ef3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_nd4l72bU8ud",
        "outputId": "afa58727-49b5-4b83-8753-0b9ad3d10227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTs_8JUQYso8",
        "outputId": "0de978f8-2ae7-480b-fb66-742a0cf62f47",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Access the secret using userdata.get()\n",
        "my_variable = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# You can also set it as an environment variable for use with os.getenv()\n",
        "os.environ['GROQ_API_KEY'] = my_variable"
      ],
      "metadata": {
        "id": "Vb5869F-b-ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiNNNjSeV0RI",
        "outputId": "d6df2b2a-a998-412b-b838-c32f9eb6b637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-text-splitters"
      ],
      "metadata": {
        "id": "hMgYXkDWbxHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"./sample_data/langchain.txt\");\n",
        "docs = loader.load();"
      ],
      "metadata": {
        "id": "fQ-OU3-CITbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pp(docs[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS6_lsBnIXqH",
        "outputId": "f480a024-9499-4a1f-e7fb-774e532f5000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': './sample_data/langchain.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-d7NVMuIjo7",
        "outputId": "1a0064c0-90ea-43d5-f9b6-ad14a94d0450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].page_content[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "t5Bxcm2DIeHB",
        "outputId": "d08333bc-ec7a-41d6-aa47-719b0caca97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain: The Definitive Framework for Building Advanced Language Model Applications\\n\\nLangChain has'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "texts = text_splitter.create_documents([docs[0].page_content])\n",
        "print(texts[0])"
      ],
      "metadata": {
        "id": "JEPHZuqpKfSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b542e195-047d-4368-dafd-e5fd958d4e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 544, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 325, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1418, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 829, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 961, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1195, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1215, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1069, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 942, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1086, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1014, which is longer than the specified 200\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 659, which is longer than the specified 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='LangChain: The Definitive Framework for Building Advanced Language Model Applications'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Example of a long document content\n",
        "long_content = \"\"\"\n",
        "This is the first paragraph. It is somewhat long, but might not be the primary cause of issues.\n",
        "\n",
        "This is the second paragraph. This paragraph is intentionally made to be much, much longer than your specified chunk_size of 200 characters. It will continue on and on, demonstrating what happens when a natural split point (like a double newline) occurs, but the content between those natural splits is excessively large. This is a common scenario with articles, technical documentation, or prose where paragraphs can span hundreds or even thousands of characters. The CharacterTextSplitter's main job is to cut at the designated separator. If it finds a large block of text that doesn't contain that specific separator within it, it will keep that entire block as a single chunk, even if it vastly exceeds the desired chunk_size. This behavior is by design for this specific splitter, as its name implies, it's about splitting by *characters* (the separator), not strictly by *size*. Other splitters like RecursiveCharacterTextSplitter are designed to be more aggressive in breaking down chunks to meet size constraints, using a hierarchy of separators. Understanding this distinction is key to choosing the right tool for your text splitting needs in LangChain. This lengthy paragraph will undoubtedly cause the warning to appear several times, as it represents a single, indivisible unit according to the \"\\n\\n\" separator.\n",
        "\n",
        "This is the third paragraph, which is also quite long, to further illustrate the point. It ensures that there are multiple instances where the splitter will be forced to create chunks larger than the desired size because the content between the defined separators is extensive. This helps solidify the understanding that while chunk_size is a guideline, the separator acts as a hard boundary that the CharacterTextSplitter respects above all else.\n",
        "\"\"\"\n",
        "\n",
        "# Simulate a Document object\n",
        "docs = [Document(page_content=long_content, metadata={\"source\": \"example\"})]\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "texts = text_splitter.create_documents([docs[0].page_content])\n",
        "\n",
        "print(\"--- Chunks created ---\")\n",
        "for i, text_doc in enumerate(texts):\n",
        "    print(f\"Chunk {i+1} (length: {len(text_doc.page_content)}):\")\n",
        "    print(text_doc.page_content[:100] + \"...\") # Print first 100 chars for brevity\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC2kQc_zlt-4",
        "outputId": "2daac6da-c975-4336-ab7e-219e530c25f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1309, which is longer than the specified 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chunks created ---\n",
            "Chunk 1 (length: 95):\n",
            "This is the first paragraph. It is somewhat long, but might not be the primary cause of issues....\n",
            "------------------------------\n",
            "Chunk 2 (length: 1309):\n",
            "This is the second paragraph. This paragraph is intentionally made to be much, much longer than your...\n",
            "------------------------------\n",
            "Chunk 3 (length: 12):\n",
            "\" separator....\n",
            "------------------------------\n",
            "Chunk 4 (length: 447):\n",
            "This is the third paragraph, which is also quite long, to further illustrate the point. It ensures t...\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        ")\n",
        "recursive_texts = recursive_splitter.create_documents([docs[0].page_content])\n",
        "\n",
        "for i, text_doc in enumerate(recursive_texts):\n",
        "    print(f\"Recursive Chunk {i+1} (length: {len(text_doc.page_content)}):\")\n",
        "    print(text_doc.page_content[:100] + \"...\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELQUayXUqY05",
        "outputId": "46fdde7f-8885-42b7-e7b4-1aed7152695a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recursive Chunk 1 (length: 95):\n",
            "This is the first paragraph. It is somewhat long, but might not be the primary cause of issues....\n",
            "------------------------------\n",
            "Recursive Chunk 2 (length: 195):\n",
            "This is the second paragraph. This paragraph is intentionally made to be much, much longer than your...\n",
            "------------------------------\n",
            "Recursive Chunk 3 (length: 189):\n",
            "what happens when a natural split point (like a double newline) occurs, but the content between thos...\n",
            "------------------------------\n",
            "Recursive Chunk 4 (length: 199):\n",
            "articles, technical documentation, or prose where paragraphs can span hundreds or even thousands of ...\n",
            "------------------------------\n",
            "Recursive Chunk 5 (length: 194):\n",
            "If it finds a large block of text that doesn't contain that specific separator within it, it will ke...\n",
            "------------------------------\n",
            "Recursive Chunk 6 (length: 186):\n",
            "chunk_size. This behavior is by design for this specific splitter, as its name implies, it's about s...\n",
            "------------------------------\n",
            "Recursive Chunk 7 (length: 197):\n",
            "splitters like RecursiveCharacterTextSplitter are designed to be more aggressive in breaking down ch...\n",
            "------------------------------\n",
            "Recursive Chunk 8 (length: 196):\n",
            "this distinction is key to choosing the right tool for your text splitting needs in LangChain. This ...\n",
            "------------------------------\n",
            "Recursive Chunk 9 (length: 62):\n",
            "as it represents a single, indivisible unit according to the \"...\n",
            "------------------------------\n",
            "Recursive Chunk 10 (length: 12):\n",
            "\" separator....\n",
            "------------------------------\n",
            "Recursive Chunk 11 (length: 199):\n",
            "This is the third paragraph, which is also quite long, to further illustrate the point. It ensures t...\n",
            "------------------------------\n",
            "Recursive Chunk 12 (length: 198):\n",
            "larger than the desired size because the content between the defined separators is extensive. This h...\n",
            "------------------------------\n",
            "Recursive Chunk 13 (length: 79):\n",
            "acts as a hard boundary that the CharacterTextSplitter respects above all else....\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### When to Use `CharacterTextSplitter` and Its Alternatives\n",
        "\n",
        "**`CharacterTextSplitter`** is like a simple pair of scissors for your text. You tell it exactly where to cut (e.g., at every double newline `\"\\n\\n\"` for paragraphs, or every comma `,` in a list).\n",
        "\n",
        "**Use `CharacterTextSplitter` when:**\n",
        "\n",
        "* **You have very specific, clear breaking points:** Imagine a document where `\"\\n\\n\"` always means a new, self-contained section, and you absolutely *do not* want to break those sections.\n",
        "* **Working with structured data:** Like processing a simple CSV where each line is a record, or log files where each entry is separated by a unique character sequence.\n",
        "* **Small documents or less strict size needs:** If the document is small, or you're okay with chunks being much larger than your `chunk_size` if a natural break point doesn't appear soon enough.\n",
        "    * **_Important Note:_** If a piece of text *between* your chosen separators is very long, `CharacterTextSplitter` will keep it as one big chunk, even if it goes way over your desired `chunk_size`. It prioritizes the separator over the size limit.\n",
        "\n",
        "---\n",
        "\n",
        "#### When NOT to Use `CharacterTextSplitter` (and What to Use Instead)\n",
        "\n",
        "For most situations, especially when preparing text for **Large Language Models (LLMs)** like in **Retrieval-Augmented Generation (RAG)**, `CharacterTextSplitter` often isn't the best fit because of its strict reliance on a single separator.\n",
        "\n",
        "**Instead, consider these for better results:**\n",
        "\n",
        "1.  **`RecursiveCharacterTextSplitter` (Your Go-To Choice!):**\n",
        "    * **Why it's better:** Think of this as a smart pair of scissors with multiple blades. It tries to cut using a list of separators (e.g., first by `\"\\n\\n\"`, then by `\"\\n\"`, then by spaces `\" \"`, then by individual characters `\"\"`).\n",
        "    * **Niche Use:** **General-purpose text splitting.** This is the recommended splitter for most documents (articles, books, reports) because it tries harder to keep chunks close to your `chunk_size` while still respecting natural breaks in the text.\n",
        "\n",
        "2.  **`MarkdownHeaderTextSplitter`:**\n",
        "    * **Niche Use:** **Markdown documents.** If your text is in Markdown format (using `#`, `##`, `###` for headers), this splitter understands that structure and will split your document based on these headings, keeping sections logically together.\n",
        "\n",
        "3.  **`TokenTextSplitter`:**\n",
        "    * **Niche Use:** When you need to split text based on the **number of tokens** (which is how LLMs measure input size) rather than raw characters. This is useful for precise control over LLM input. It's especially handy when working directly with a tokenizer from an LLM.\n",
        "\n",
        "---\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "* Use `CharacterTextSplitter` for very structured data or when you absolutely must split only at specific characters.\n",
        "* For everything else, especially with LLMs and RAG, `RecursiveCharacterTextSplitter` is usually your best friend.\n",
        "* Use specialized splitters like `MarkdownHeaderTextSplitter` or `TokenTextSplitter` when your data has a specific format or when token-based splitting is critical."
      ],
      "metadata": {
        "id": "BrbGNpw2s9Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DVD_bSyIrL9a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}