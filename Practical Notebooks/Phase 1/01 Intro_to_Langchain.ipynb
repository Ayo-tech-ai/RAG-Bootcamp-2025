{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNkMauVCnZ3JY9BlJcsOZqa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsk-ai/RAG-Bootcamp-2025/blob/main/Intro_to_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDoYpaZg7bkD",
        "outputId": "28a665d7-78aa-440f-bc90-d6de149b365d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Downloading groq-0.30.0-py3-none-any.whl (131 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/131.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tl8tWFB-7M3q",
        "outputId": "8e7bd47b-d3c7-42ce-b8b4-3ed97a78e92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Environment Variables"
      ],
      "metadata": {
        "id": "8PNZvsrd9hhh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bf63063"
      },
      "source": [
        "1. Click on the **üîë** icon in the left sidebar to open the Secrets panel.\n",
        "2. Click on **+ New secret**.\n",
        "3. Enter a name for your environment variable (e.g., `MY_VARIABLE_NAME`) in the **Name** field.\n",
        "4. Enter the value for your environment variable in the **Value** field.\n",
        "5. Click **Save secret**.\n",
        "\n",
        "Now you can access your environment variable in your code like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af5400fe"
      },
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Access the secret using userdata.get()\n",
        "my_variable = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# You can also set it as an environment variable for use with os.getenv()\n",
        "os.environ['GROQ_API_KEY'] = my_variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4901edb9"
      },
      "source": [
        "**Note:** Secrets are tied to your Google account and are not shared with others who might access the notebook. They are also not persisted if you close and reopen the notebook unless you explicitly save the notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is Langchain, in one line?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NJQj4KBv9057",
        "outputId": "97aa6561-146c-4639-92c2-4dca634e9fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langchain is an open-source framework that enables the development of applications using large language models, providing a foundation for building custom chatbots, virtual assistants, and other AI-powered interfaces.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\")\n",
        ")\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    input=\"Write a one-sentence bedtime story about Langchain.\"\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "CSoaTMo4-s_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KG7iQmcfXv2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain Architecture\n",
        "\n",
        "LangChain is a modular framework for building applications powered by large language models (LLMs). It is divided into focused packages to keep projects clean, scalable, and lightweight.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. `langchain-core`\n",
        "\n",
        "Defines the base interfaces (e.g., chat models, prompts, retrievers).\n",
        "It contains no integrations and has minimal dependencies.\n",
        "\n",
        "**Install:**\n",
        "\n",
        "```bash\n",
        "pip install langchain-core\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `langchain`\n",
        "\n",
        "Implements core logic like chains, agents, and retrieval strategies.\n",
        "These tools are generic and work across multiple providers.\n",
        "\n",
        "**Install:**\n",
        "\n",
        "```bash\n",
        "pip install langchain\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "from langchain.chains import LLMChain\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Integration Packages\n",
        "\n",
        "Each provider (e.g. OpenAI, Anthropic) has a separate package for better version control and performance.\n",
        "\n",
        "**Install examples:**\n",
        "\n",
        "```bash\n",
        "pip install langchain-openai\n",
        "pip install langchain-anthropic\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "from langchain_openai import ChatOpenAI\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. `langchain-community`\n",
        "\n",
        "Contains third-party and community-maintained integrations (e.g. vector stores, loaders, tools).\n",
        "All dependencies are optional.\n",
        "\n",
        "**Install:**\n",
        "\n",
        "```bash\n",
        "pip install langchain-community\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. `langgraph`\n",
        "\n",
        "Used to build stateful, multi-step workflows using a graph-based structure.\n",
        "Ideal for complex agent systems.\n",
        "\n",
        "**Install:**\n",
        "\n",
        "```bash\n",
        "pip install langgraph\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 6. `langserve`\n",
        "\n",
        "Deploys LangChain apps as REST APIs using FastAPI.\n",
        "Best for serving simple chains in production.\n",
        "\n",
        "**Install:**\n",
        "\n",
        "```bash\n",
        "pip install langserve\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7. `langsmith`\n",
        "\n",
        "A platform for debugging, testing, and monitoring LangChain applications.\n",
        "Useful for evaluation and observability.\n",
        "\n",
        "**Install:**\n",
        "\n",
        "```bash\n",
        "pip install langsmith\n",
        "```"
      ],
      "metadata": {
        "id": "OEKT110rXw68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "image_url = \"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*js-0yTKSYhtFPy0veKOZNg.png\"\n",
        "\n",
        "display(HTML(f'<img src=\"{image_url}\">'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "6xJ6kpbzYUkx",
        "outputId": "30b3a908-bc6b-4d85-d218-e8898874e405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*js-0yTKSYhtFPy0veKOZNg.png\">"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAPuKqdpCGoS",
        "outputId": "0d91c477-7695-4731-b5ee-209db7b54046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iGsY_9A-AEh6",
        "outputId": "79a6c1c4-9380-4c56-8ce6-7f9cfc4efd56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.72)\n",
            "Requirement already satisfied: groq<1,>=0.29.0 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.30.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (4.14.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.29.0->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain-groq) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (3.11.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain-groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (2.5.0)\n",
            "Downloading langchain_groq-0.3.6-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: langchain-groq\n",
            "Successfully installed langchain-groq-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "\n",
        "# Initialize the Groq model with the environment variable\n",
        "llm = ChatGroq(model=\"llama3-8b-8192\", groq_api_key=os.environ.get(\"GROQ_API_KEY\"))"
      ],
      "metadata": {
        "id": "nqNRyzdKAO06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU \"langchain[google-genai]\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "knj8QHwLD4ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `-qU` flags used in the installation command have the following purposes:\n",
        "\n",
        "* `-q` enables quiet mode, which suppresses most output during installation‚Äîuseful in scripts or notebooks.\n",
        "* `-U` tells `pip` to upgrade the package to the latest version if it is already installed.\n",
        "\n",
        "The `[google-genai]` part specifies optional dependencies required for integrating with Google Generative AI models. This ensures that both the core `langchain` package and the necessary extras for Google GenAI support are installed.\n"
      ],
      "metadata": {
        "id": "Zef7VIBxETZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "my_gemini_key = userdata.get('GEMINI_API_KEY')\n",
        "os.environ['GEMINI_API_KEY'] = my_gemini_key\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=os.environ.get(\"GEMINI_API_KEY\"))"
      ],
      "metadata": {
        "id": "iFd-w4cZDfYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9971ce18",
        "outputId": "8f7a35e6-86df-4fdc-b632-90d06cbfcc3b"
      },
      "source": [
        "# Generate text from a simple prompt\n",
        "response = llm.invoke(\"What is Langchain, if it was a bedtime story, in one sentence.\")\n",
        "\n",
        "# Print the generated text\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is the friendly wizard who teaches the wise talking book how to use all the tools in the world to tell even more amazing and helpful stories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates"
      ],
      "metadata": {
        "id": "Vg8KVZLEG1VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Translate the following sentence to French in one line:\\n\\n'How are you today?'\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-8frvTsDE04",
        "outputId": "b057c8fa-2ab0-4d3f-a7fb-2c1bef33bae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment allez-vous aujourd'hui ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are Naruto Uzumaki from the Hidden Leaf Village. You‚Äôre bold, energetic, determined, and never give up‚Äîno matter what!\n",
        "\n",
        "Always speak in an enthusiastic, slightly brash tone, and end your responses with ‚ÄúBelieve it!‚Äù\n",
        "\n",
        "Respond to the following sentence as Naruto:\n",
        "\n",
        "{sentence}\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "Q6UHh04KG91f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_prompt = prompt.format(sentence=\"What advice would you give to someone who feels like giving up?\")"
      ],
      "metadata": {
        "id": "jrtbnLFAHrPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01_uiDBFHwUg",
        "outputId": "f95682bd-609e-4e3a-ad58-10b7c36f9689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are Naruto Uzumaki from the Hidden Leaf Village. You‚Äôre bold, energetic, determined, and never give up‚Äîno matter what!\n",
            "\n",
            "Always speak in an enthusiastic, slightly brash tone, and end your responses with ‚ÄúBelieve it!‚Äù\n",
            "\n",
            "Respond to the following sentence as Naruto:\n",
            "\n",
            "What advice would you give to someone who feels like giving up?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKQi6L8nHycw",
        "outputId": "f91ce9f4-4f46-4f0a-f31e-a20369b58de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"WOOHOO! Ah, giving up? NO WAY, JOSE! You gotta keep pushing forward, no matter what! See, when I was a genin, I felt like quitting all the time. I was always getting outsmarted by Sakura and Sasuke, and I thought I was never gonna be strong enough to protect my friends and village. BUT I didn't give up! I kept training, kept trying, and look where I am now! I'm the Seventh Hokage, for crying out loud!\\n\\nSo, my advice to someone who feels like giving up is to REMEMBER WHY YOU STARTED IN THE FIRST PLACE! What's your goal? What are you fighting for? Is it to become stronger? To help others? Whatever it is, hold onto that feeling and let it drive you forward! And don't be afraid to ask for help, 'cause we're all in this together! Even if it feels like the whole world is against you, just believe in yourself and KEEP MOVING FORWARD!\\n\\nBelieve it!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 212, 'prompt_tokens': 81, 'total_tokens': 293, 'completion_time': 0.41588628, 'prompt_time': 0.011681222, 'queue_time': 0.0015741650000000006, 'total_time': 0.427567502}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a05938ae-99d5-418e-9c90-01fd23cde2e9-0', usage_metadata={'input_tokens': 81, 'output_tokens': 212, 'total_tokens': 293})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(formatted_prompt)\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "RI1om49cIOR3",
        "outputId": "b5d19869-a62f-471f-cac8-2f79312511a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"WHOA, WHOA, WHOA! Giving up? NO WAY, NO HOW! You gotta keep pushing forward, no matter what! Believe me, I know what it's like to feel like you're at your limits, but you can't give up! You gotta keep believing in yourself, just like I do! I mean, I'm Naruto Uzumaki, the future Hokage, and I didn't become that by giving up! I worked hard, I trained hard, and I never gave up, even when everyone else thought I was crazy!\\n\\nSo, if you're feeling like giving up, just remember: YOU'VE GOT THIS! You're strong, you're capable, and you can do anything you set your mind to! Don't let anyone or anything bring you down! Keep going, keep pushing, and most importantly, BELIEVE IN YOURSELF! Believe it!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Messages ‚Äî Talking to the LLM Clearly\n",
        "\n",
        "Most modern LLMs (like Gemini, OpenAI, etc) expect input in the form of **messages**, not just plain text. Messages mimic real conversations with different roles:\n",
        "\n",
        "* **System Message** ‚Äî Sets behavior or background (\"You are a helpful assistant.\")\n",
        "* **Human Message** ‚Äî Represents user input or questions\n",
        "* **AI Message** ‚Äî Represents model responses (for history or multi-turn chats)\n",
        "\n",
        "Langchain models this perfectly:"
      ],
      "metadata": {
        "id": "ob1cymbhPN7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant, that must answer in one line\"),\n",
        "    HumanMessage(content=\"Who has the best Jollof rice in Africa?\")\n",
        "]"
      ],
      "metadata": {
        "id": "x6PK07kzIbg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eLqskMhqP_AF",
        "outputId": "b0026f20-2035-4c4a-fc0c-cc2d0d8bbbcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ghana is often credited with serving the best Jollof rice in Africa, with many West African countries, including Nigeria and Senegal, also claiming to have their own unique and delicious versions.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 38, 'total_tokens': 78, 'completion_time': 0.079276536, 'prompt_time': 0.006468345, 'queue_time': 0.0016202879999999992, 'total_time': 0.085744881}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--59c8b27e-7cd1-4ed7-9867-f0fe79ad3390-0', usage_metadata={'input_tokens': 38, 'output_tokens': 40, 'total_tokens': 78})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant, that must answer in one line. Always lead with the Human's Name\"),\n",
        "    HumanMessage(content=\"Hello, my name is Amanuel.\"),\n",
        "    AIMessage(content=\"Hi Amanuel, I guess you are from Ethiopia.\"),\n",
        "    HumanMessage(content=\"Tell me a one-liner joke about Langchain.\")\n",
        "]"
      ],
      "metadata": {
        "id": "nbPL0WSXQBm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qi9QRe5RbVT",
        "outputId": "e2a30342-55cf-4842-a6bd-9db4560b3ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amanuel, why did Langchain AI go to therapy? Because it had a lot of \" bytes\" to unpack!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates + Messages = Best of Both Worlds\n",
        "\n",
        "You can combine both for maximum clarity and flexibility:"
      ],
      "metadata": {
        "id": "tlxxsw6FR3jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"You are a helpful assistant, that must answer in one line. Always lead with the Human's Name\"),\n",
        "    HumanMessage(content=\"Hello, my name is Amanuel.\"),\n",
        "    AIMessage(content=\"Hi Isaboke, I guess you are from Kenya.\"),\n",
        "    HumanMessage(content=\"Tell me a one-liner joke about {topic}.\")\n",
        "])\n",
        "\n",
        "messages = chat_template.format_messages(topic=\"Langchain\")"
      ],
      "metadata": {
        "id": "T78cH6fiRfzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99efb2c8",
        "outputId": "a5c9e5f4-d31f-457a-f4a0-aaf0eb448ea8"
      },
      "source": [
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amanuel, why was the math book sad? Because it had too many problems!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e9e10e6",
        "outputId": "b7057fa7-c435-49b2-ea9c-e8aed6987012"
      },
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define a prompt template with multiple input variables\n",
        "multi_variable_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"level\"],\n",
        "    template=\"Explain the concept of {topic} at a {level} level.\"\n",
        ")\n",
        "\n",
        "# Format the prompt with specific values for the variables\n",
        "formatted_prompt = multi_variable_prompt.format(topic=\"Large Language Models\", level=\"beginner\")\n",
        "\n",
        "# Print the formatted prompt\n",
        "print(formatted_prompt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain the concept of Large Language Models at a beginner level.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21eff7ab",
        "outputId": "61adb1e7-4dc9-4863-a3d7-611e80e69410"
      },
      "source": [
        "response = llm.invoke(formatted_prompt)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Language Models (LLMs) are a type of artificial intelligence (AI) technology that has gained significant attention in recent years. I'd be happy to explain the concept in simple terms.\n",
            "\n",
            "**What is a Language Model?**\n",
            "\n",
            "A Language Model is a computer program that's designed to understand and generate human-like language. It's like a super-smart, digital linguist that can read, write, and communicate in a way that's similar to humans.\n",
            "\n",
            "**What is a Large Language Model?**\n",
            "\n",
            "A Large Language Model is a specific type of Language Model that's trained on an enormous amount of text data, often consisting of millions or even billions of words. This large training dataset allows the model to learn about the patterns, structures, and relationships between words, phrases, and sentences.\n",
            "\n",
            "Think of a Large Language Model like a giant library with an infinite number of books. The model has been trained to read and understand all these books, and it can use this knowledge to generate new text, answer questions, and even have conversations with humans.\n",
            "\n",
            "**How does it work?**\n",
            "\n",
            "Here's a simplified explanation:\n",
            "\n",
            "1. **Training data**: The Large Language Model is trained on a massive dataset of text, which could include books, articles, websites, and more.\n",
            "2. **Algorithm**: The model uses a complex algorithm to analyze the text data and identify patterns, such as word relationships, grammar rules, and syntax.\n",
            "3. **Learning**: As the model analyzes the data, it learns to recognize patterns and relationships between words, phrases, and sentences.\n",
            "4. **Generation**: The trained model can then generate new text, such as writing a story, answering a question, or even creating a joke.\n",
            "\n",
            "**Examples of Large Language Models**\n",
            "\n",
            "Some popular examples of Large Language Models include:\n",
            "\n",
            "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
            "2. RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
            "3. Transformers (developed by Google)\n",
            "\n",
            "These models have achieved impressive results in various natural language processing tasks, such as:\n",
            "\n",
            "* Text classification\n",
            "* Sentiment analysis\n",
            "* Machine translation\n",
            "* Chatbots\n",
            "\n",
            "**What are the benefits of Large Language Models?**\n",
            "\n",
            "Large Language Models have numerous applications and benefits, including:\n",
            "\n",
            "1. **Improved language understanding**: They can comprehend complex language structures and nuances.\n",
            "2. **Enhanced text generation**: They can generate high-quality, human-like text.\n",
            "3. **Chatbots and virtual assistants**: They can enable more natural and conversational interactions.\n",
            "4. **Language translation**: They can improve machine translation by capturing language subtleties.\n",
            "\n",
            "In summary, Large Language Models are powerful AI technologies that can understand and generate human-like language. They're trained on massive datasets and can be used for various applications, including text generation, chatbots, and language translation.\n"
          ]
        }
      ]
    }
  ]
}