{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2be000-3472-4313-b072-e0cc31b0890e",
   "metadata": {},
   "source": [
    "## Section 2, Part 5: Structuring LLM Outputs with Parsers & Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19250fe-89e0-45e6-ac83-18225016fe27",
   "metadata": {},
   "source": [
    "#### **1. What Are Output Parsers and Why Do We Need Them?**\n",
    "\n",
    "Imagine you ask a Large Language Model (LLM) a question. The LLM, at its core, gives you back a string of text.\n",
    "\n",
    "* **You ask:** \"Who were the first three astronauts to land on the moon?\"\n",
    "* **LLM returns (as a string):** \"The first three astronauts to land on the moon were Neil Armstrong, Buzz Aldrin, and Michael Collins.\"\n",
    "\n",
    "This is great for a human to read. But what if your application needs that information in a structured format, like a Python list, to do something with it?\n",
    "\n",
    "```python\n",
    "# What your application needs:\n",
    "[\"Neil Armstrong\", \"Buzz Aldrin\", \"Michael Collins\"]\n",
    "```\n",
    "\n",
    "This is where **Output Parsers** come in. They are special classes in LangChain whose job is to take the raw text output from an LLM and convert it into a more useful, structured format that your code can easily work with.\n",
    "\n",
    "Think of them as the bridge between the LLM's free-form text and your application's structured data requirements.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Common Types of Output Parsers (with Code)**\n",
    "\n",
    "Let's explore the most common parsers you'll use.\n",
    "\n",
    "##### **a) `StrOutputParser` - The Default**\n",
    "\n",
    "This is the most basic parser. It doesn't do much! It simply takes the output from the model and converts it into a standard Python `string`. It's the default final step in many simple chains.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Define our components\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short, one-sentence joke about {topic}.\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 2. Create the chain\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 3. Invoke the chain\n",
    "result = chain.invoke({\"topic\": \"computers\"})\n",
    "\n",
    "# 4. Print the result and its type\n",
    "print(result)\n",
    "print(f\"\\nType of result: {type(result)}\")\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Why did the computer keep sneezing? It had a virus!\n",
    "\n",
    "Type of result: <class 'str'>\n",
    "```\n",
    "\n",
    "##### **b) `JsonOutputParser` - For Dictionaries and JSON**\n",
    "\n",
    "This is incredibly useful when you need the LLM to return multiple pieces of information. You instruct the model to format its response as a JSON object, and this parser will automatically convert that JSON string into a Python dictionary.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# 1. Instantiate the parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# 2. Create a prompt that instructs the model to output JSON\n",
    "#    We use the parser's .get_format_instructions() method to help the model\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Extract information from the following text.\\n\"\n",
    "    \"{format_instructions}\\n\"\n",
    "    \"Text: {text}\"\n",
    ")\n",
    "\n",
    "# 3. Create the model and chain\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 4. Invoke the chain\n",
    "text_to_parse = \"The Eiffel Tower was built in 1889 and is located in Paris, France. It is 330 meters tall.\"\n",
    "result = chain.invoke({\n",
    "    \"text\": text_to_parse,\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "# 5. Print the result and its type\n",
    "print(result)\n",
    "print(f\"\\nType of result: {type(result)}\")\n",
    "print(f\"The landmark is {result['landmark']} located in {result['location']}.\")\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "```json\n",
    "{'landmark': 'Eiffel Tower', 'year_built': 1889, 'location': 'Paris, France', 'height_meters': 330}\n",
    "\n",
    "Type of result: <class 'dict'>\n",
    "The landmark is Eiffel Tower located in Paris, France.\n",
    "```\n",
    "\n",
    "##### **c) `PydanticOutputParser` - The Most Robust Method**\n",
    "\n",
    "This is the most powerful and recommended approach for structured data. It uses the `Pydantic` library to let you define a data schema with types. The parser then does two things:\n",
    "\n",
    "1.  It generates highly specific formatting instructions for the LLM based on your schema.\n",
    "2.  It parses the LLM's output into a Pydantic object, which gives you type-hinting, validation, and easy access to your data.\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# 1. Define your desired data structure with Pydantic\n",
    "class Recipe(BaseModel):\n",
    "    name: str = Field(description=\"The name of the recipe\")\n",
    "    ingredients: List[str] = Field(description=\"A list of ingredients for the recipe\")\n",
    "    servings: int = Field(description=\"The number of people the recipe serves\")\n",
    "\n",
    "# 2. Set up a parser with your Pydantic object\n",
    "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "# 3. Create a prompt with the format instructions\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a helpful assistant that generates recipes based on a user's request.\\n\"\n",
    "    \"{format_instructions}\\n\"\n",
    "    \"User request: {request}\\n\"\n",
    ")\n",
    "\n",
    "# 4. Create the model and chain\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 5. Invoke the chain\n",
    "request = \"Give me a simple recipe for a classic spaghetti bolognese.\"\n",
    "result = chain.invoke({\n",
    "    \"request\": request,\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "# 6. Print the result and its type\n",
    "print(result)\n",
    "print(f\"\\nType of result: {type(result)}\")\n",
    "print(f\"\\nRecipe for: {result.name}\")\n",
    "print(f\"It serves: {result.servings}\")\n",
    "print(\"Ingredients:\")\n",
    "for ingredient in result.ingredients:\n",
    "    print(f\"- {ingredient}\")\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "name='Classic Spaghetti Bolognese' ingredients=['Ground Beef', 'Onion', 'Garlic', 'Canned Tomatoes', 'Tomato Paste', 'Spaghetti', 'Olive Oil', 'Salt', 'Pepper'] servings=4\n",
    "\n",
    "Type of result: <class '__main__.Recipe'>\n",
    "\n",
    "Recipe for: Classic Spaghetti Bolognese\n",
    "It serves: 4\n",
    "Ingredients:\n",
    "- Ground Beef\n",
    "- Onion\n",
    "- Garlic\n",
    "- Canned Tomatoes\n",
    "- Tomato Paste\n",
    "- Spaghetti\n",
    "- Olive Oil\n",
    "- Salt\n",
    "- Pepper\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary**\n",
    "\n",
    "* **LLMs output strings.** Your application often needs structured data (lists, JSON, objects).\n",
    "* **Output Parsers** are the components that convert the LLM's string output into the structured format you need.\n",
    "* **`StrOutputParser`** is the simplest, giving you a plain string.\n",
    "* **`JsonOutputParser`** is great for getting back Python dictionaries.\n",
    "* **`PydanticOutputParser`** is the most robust, giving you validated data objects with types, making your application more reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca9ed4-04a0-4d29-bd2a-eb534109bc70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
